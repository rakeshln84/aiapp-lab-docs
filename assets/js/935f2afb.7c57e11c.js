"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[696],{5988:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Welcome","href":"/aiapp-lab-docs/","docId":"Welcome","unlisted":false},{"type":"link","label":"Get started","href":"/aiapp-lab-docs/setup","docId":"Get-Started","unlisted":false},{"type":"category","label":" Part 1 - Labs","collapsible":true,"collapsed":false,"className":"red","items":[{"type":"link","label":"Basic Prompting","href":"/aiapp-lab-docs/Part-1-labs/Basic-Prompting","docId":"Part-1-labs/Basic-Prompting","unlisted":false},{"type":"link","label":"Prompt engineering techniques","href":"/aiapp-lab-docs/Part-1-labs/Prompt-engineering-techniques","docId":"Part-1-labs/Prompt-engineering-techniques","unlisted":false}]},{"type":"category","label":" Part 2 - Labs","collapsible":true,"collapsed":false,"className":"red","items":[{"type":"link","label":"System Message","href":"/aiapp-lab-docs/Part-2-labs/System-Message","docId":"Part-2-labs/System-Message","unlisted":false},{"type":"link","label":"Add knowledge","href":"/aiapp-lab-docs/Part-2-labs/Add-Knowledge","docId":"Part-2-labs/Add-Knowledge","unlisted":false},{"type":"link","label":"Function Calling","href":"/aiapp-lab-docs/Part-2-labs/Function-Calling","docId":"Part-2-labs/Function-Calling","unlisted":false}]},{"type":"category","label":"Concepts & resources","collapsible":true,"collapsed":true,"className":"red","items":[{"type":"link","label":"AI Models & Deployments","href":"/aiapp-lab-docs/ai-models","docId":"Concepts/Explore-Models","unlisted":false},{"type":"link","label":"Large Language Model (LLM)","href":"/aiapp-lab-docs/llms","docId":"Concepts/Understand-LLMs","unlisted":false},{"type":"link","label":"At home","href":"/aiapp-lab-docs/at-home","docId":"Concepts/At-home","unlisted":false},{"type":"link","label":"Tokenization","href":"/aiapp-lab-docs/tokenization","docId":"Concepts/Understand-Tokens","unlisted":false}]},{"type":"link","label":"Summary","href":"/aiapp-lab-docs/summary","docId":"Summary","unlisted":false}]},"docs":{"Concepts/At-home":{"id":"Concepts/At-home","title":"At home","description":"At home","sidebar":"tutorialSidebar"},"Concepts/Explore-Models":{"id":"Concepts/Explore-Models","title":"AI Models & Deployments","description":"What is an AI Model?","sidebar":"tutorialSidebar"},"Concepts/Understand-LLMs":{"id":"Concepts/Understand-LLMs","title":"Large Language Model (LLM)","description":"A large language model (LLM) is a type of AI that can process and produce natural language text. It learns from a massive amount of text data such as books, articles, and web pages to discover patterns and rules of language from them.","sidebar":"tutorialSidebar"},"Concepts/Understand-Tokens":{"id":"Concepts/Understand-Tokens","title":"Tokenization","description":"We\'ve mentioned \\"tokens\\" a few times in previous lessons, but we didn\'t explain what those were and why they matter. Let\'s discuss that now.","sidebar":"tutorialSidebar"},"Get-Started":{"id":"Get-Started","title":"Get started","description":"- Use your own laptop.","sidebar":"tutorialSidebar"},"Part-1-labs/Basic-Prompting":{"id":"Part-1-labs/Basic-Prompting","title":"Basic Prompting","description":"Prompt engineering is a concept in Natural Language Processing (NLP) that involves embedding descriptions of tasks in input to prompt the model to output the desired results.","sidebar":"tutorialSidebar"},"Part-1-labs/Prompt-engineering-techniques":{"id":"Part-1-labs/Prompt-engineering-techniques","title":"Prompt engineering techniques","description":"OpenAI models like GPT-3 do not learn or adapt during user interactions. They generate responses based on pre-training with a large dataset and do not update their knowledge from individual conversations. Any improvements or updates to the model\'s capabilities are made through a controlled retraining process by OpenAI, not through real-time learning.","sidebar":"tutorialSidebar"},"Part-2-labs/Add-Knowledge":{"id":"Part-2-labs/Add-Knowledge","title":"Add knowledge","description":"Retrieval-Augmented Generation (RAG) is an AI technique that retrieves relevant information from a database and then uses it to help generate more informed and contextually accurate text responses.","sidebar":"tutorialSidebar"},"Part-2-labs/Function-Calling":{"id":"Part-2-labs/Function-Calling","title":"Function Calling","description":"GPT-3.5 and GPT-4 models can take user-defined functions as input and generate structured output.","sidebar":"tutorialSidebar"},"Part-2-labs/System-Message":{"id":"Part-2-labs/System-Message","title":"System Message","description":"The system message is used to communicate instructions or provide context to the model at the beginning of a conversation.","sidebar":"tutorialSidebar"},"Summary":{"id":"Summary","title":"Summary","description":"We hope that in the last hour, you\'ve learned what natural language generative AI models are and how they work and understand the power of prompt-engineering.","sidebar":"tutorialSidebar"},"Welcome":{"id":"Welcome","title":"Welcome","description":"This is a 75-minute workshop that will give you a hands-on introduction to the core concepts and best practices for interacting with OpenAI models.","sidebar":"tutorialSidebar"}}}')}}]);